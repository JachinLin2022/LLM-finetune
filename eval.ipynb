{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eceb5db-e925-4071-a2f9-693ed811aff2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /root/miniconda3/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /root/miniconda3/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('Asia/Shanghai')}\n",
      "  warn(msg)\n",
      "/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "\"\"\"\n",
    "Unused imports:\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "\"\"\"\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from peft import PeftModel\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "from utils.prompter import Prompter\n",
    "# lora hyperparams\n",
    "lora_r = 8\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.05\n",
    "lora_target_modules = [\n",
    "    \"q_proj\",\n",
    "    \"v_proj\",\n",
    "]\n",
    "\n",
    "prompt_template_name = \"alpaca\"  # The prompt template to use, will default to alpaca.\n",
    "device_map = \"auto\"\n",
    "\n",
    "base_model = \"/root/llama-7b-hf\"  # the only required argument\n",
    "data_path = \"train_data_3_class_clean.jsonl\"\n",
    "output_dir = \"/root/autodl-tmp/output\"\n",
    "lora_weights = \"/root/autodl-tmp/checkpoint-5800\"\n",
    "# resume_from_checkpoint = \"/root/autodl-tmp/output/checkpoint-3700\"\n",
    "resume_from_checkpoint=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5724de47-77e7-42d6-9dd0-cb0b7432e5b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompter = Prompter(prompt_template_name)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "\n",
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    " )\n",
    "tokenizer.padding_side = \"left\"  # Allow batched inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45e4486b-21c5-46b4-a66b-8173ffe020e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45205c926e064cc5bcd34d62ffe44a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        load_in_8bit=False,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=device_map,\n",
    "    )\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    lora_weights,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f19369d4-9660-4b76-9cd2-f70222f95885",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 0.9873, 1.0000, 1.0000, 1.0000, 1.0000, 0.7075, 0.9995],\n",
      "       device='cuda:0', dtype=torch.float16) tensor([ 1066, 22198, 22198,  1066,  1066,  1066, 17821, 22198],\n",
      "       device='cuda:0')\n",
      "36.59375 pos positive\n",
      "25.765625 negative positive\n",
      "20.890625 negative negative\n",
      "35.9375 pos positive\n",
      "36.21875 pos positive\n",
      "33.5 pos positive\n",
      "28.0 neut positive\n",
      "19.125 negative negative\n",
      "6 815 0.007361963190184049\n",
      "Finished in 0.35 seconds.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# instructions = json.load(open(\"data/alpaca_data.json\"))\n",
    "instructions = []\n",
    "with open(data_path, 'r') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        data_point = {}\n",
    "        data_point[\"instruction\"] = 'What is the sentiment toward Bitcoin in the input sentence? [positive, negative, neutral]'\n",
    "        data_point[\"input\"] = data['text']\n",
    "        data_point[\"output\"] = data['label']\n",
    "        full_prompt = prompter.generate_prompt(\n",
    "            data_point[\"instruction\"],\n",
    "            data_point[\"input\"],\n",
    "        )\n",
    "        instructions.append({'context':full_prompt, 'target':data['label']})\n",
    "\n",
    "# print(instructions[0])\n",
    "\n",
    "tokenid_map = {\n",
    "    'pos': 1066,\n",
    "    'negative': 22198,\n",
    "    'neut': 17821\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    right = 0\n",
    "    all = 0\n",
    "    batch_size = 8\n",
    "    input_texts = []\n",
    "    targets = []\n",
    "    for idx, item in enumerate(instructions[300:]):\n",
    "        # feature = format_example(item)\n",
    "        # input_text = feature[\"context\"]\n",
    "        all = all + 1\n",
    "        input_texts.append(item[\"context\"])\n",
    "        targets.append(item[\"target\"])\n",
    "    test_loader = DataLoader(input_texts, batch_size=batch_size)\n",
    "    for batch_idx,batch in enumerate(test_loader):\n",
    "        input_ids = tokenizer(batch, padding=True,return_tensors='pt').to('cuda')\n",
    "        # out = model.generate(**input_ids, max_length=250, temperature=0)\n",
    "        out = model.generate(\n",
    "            **input_ids,\n",
    "            temperature=0,\n",
    "            return_dict_in_generate= True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens = 1\n",
    "        )\n",
    "        seqs = out['sequences']\n",
    "        scores = out['scores'][0]\n",
    "        softmax = F.softmax(scores, dim=1)\n",
    "        \n",
    "        pos = scores[:,1066]\n",
    "        neg = scores[:,22198]\n",
    "        neu = scores[:,17821]\n",
    "        \n",
    "        pos_prob = softmax[:,1066]\n",
    "        neg_prob = softmax[:,22198]\n",
    "        neu_prob = softmax[:,17821]\n",
    "        \n",
    "        max_val, max_index = torch.max(softmax, dim=1)\n",
    "        \n",
    "        print(max_val, max_index)\n",
    "#         print(tokenizer.decode(max_index))\n",
    "        \n",
    "        \n",
    "        results = tokenizer.batch_decode(seqs)\n",
    "        # print(results, '\\n\\n')\n",
    "        # break\n",
    "        for idx,res in enumerate(results):\n",
    "            # print(res,'\\n\\n')\n",
    "            pred = res[res.find('Response') + 10:]\n",
    "            \n",
    "            target = targets[batch_idx*batch_size + idx]\n",
    "            print(pos[idx].item(),pred, target)\n",
    "#             # print(pred, batch_idx*batch_size + idx,targets[batch_idx*batch_size + idx])\\\n",
    "            # if pred.find(target) >= 0:\n",
    "            #     right = right + 1\n",
    "            if target.find(pred) >= 0:\n",
    "                right = right + 1\n",
    "        print(right,all,right/all)\n",
    "        break\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Finished in {:.2f} seconds.\".format(elapsed_time))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ddc766-b1da-43f2-9a42-d4aaa04337d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
