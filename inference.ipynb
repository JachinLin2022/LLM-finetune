{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eceb5db-e925-4071-a2f9-693ed811aff2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /root/miniconda3/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /root/miniconda3/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('Asia/Shanghai')}\n",
      "  warn(msg)\n",
      "/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/root/LLM-finetune/Untitled.ipynb')}\n",
      "  warn(msg)\n",
      "/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "\"\"\"\n",
    "Unused imports:\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "\"\"\"\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from peft import PeftModel\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "from utils.prompter import Prompter\n",
    "# lora hyperparams\n",
    "lora_r = 8\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.05\n",
    "lora_target_modules = [\n",
    "    \"q_proj\",\n",
    "    \"v_proj\",\n",
    "]\n",
    "\n",
    "prompt_template_name = \"alpaca\"  # The prompt template to use, will default to alpaca.\n",
    "device_map = \"auto\"\n",
    "\n",
    "base_model = \"/root/llama-7b-hf\"  # the only required argument\n",
    "data_path = \"train_data_3_class_clean.jsonl\"\n",
    "output_dir = \"/root/autodl-tmp/output\"\n",
    "lora_weights = \"/root/autodl-tmp/output2s/checkpoint-100\"\n",
    "# resume_from_checkpoint = \"/root/autodl-tmp/output/checkpoint-3700\"\n",
    "resume_from_checkpoint=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5724de47-77e7-42d6-9dd0-cb0b7432e5b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompter = Prompter(prompt_template_name)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "\n",
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    " )\n",
    "tokenizer.padding_side = \"left\"  # Allow batched inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45e4486b-21c5-46b4-a66b-8173ffe020e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed93cc68c6ee43d1b5415aa2d46a332f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        load_in_8bit=False,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=device_map,\n",
    "    )\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    lora_weights,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f19369d4-9660-4b76-9cd2-f70222f95885",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat is the sentiment toward Bitcoin in the input sentence? [positive, negative, neutral]\\n\\n### Input:\\nBut... Bitcoin's halving IS priced in.\\n\\nFirst reminder of 2020\\n\\n### Response:\\n\", 'target': 'negative'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 815 0.007361963190184049\n",
      "13 815 0.015950920245398775\n",
      "18 815 0.022085889570552148\n",
      "23 815 0.02822085889570552\n",
      "31 815 0.03803680981595092\n",
      "38 815 0.046625766871165646\n",
      "45 815 0.05521472392638037\n",
      "52 815 0.0638036809815951\n",
      "59 815 0.07239263803680981\n",
      "65 815 0.07975460122699386\n",
      "73 815 0.08957055214723926\n",
      "79 815 0.09693251533742331\n",
      "87 815 0.1067484662576687\n",
      "94 815 0.11533742331288344\n",
      "99 815 0.12147239263803682\n",
      "106 815 0.13006134969325153\n",
      "112 815 0.1374233128834356\n",
      "119 815 0.1460122699386503\n",
      "127 815 0.1558282208588957\n",
      "134 815 0.16441717791411042\n",
      "140 815 0.17177914110429449\n",
      "148 815 0.18159509202453988\n",
      "152 815 0.18650306748466258\n",
      "158 815 0.19386503067484662\n",
      "165 815 0.20245398773006135\n",
      "170 815 0.2085889570552147\n",
      "177 815 0.21717791411042944\n",
      "185 815 0.22699386503067484\n",
      "193 815 0.23680981595092024\n",
      "201 815 0.24662576687116564\n",
      "209 815 0.25644171779141106\n",
      "215 815 0.26380368098159507\n",
      "223 815 0.27361963190184047\n",
      "231 815 0.28343558282208586\n",
      "235 815 0.2883435582822086\n",
      "240 815 0.294478527607362\n",
      "248 815 0.3042944785276074\n",
      "256 815 0.3141104294478528\n",
      "261 815 0.3202453987730061\n",
      "268 815 0.32883435582822085\n",
      "276 815 0.33865030674846625\n",
      "284 815 0.34846625766871164\n",
      "289 815 0.35460122699386504\n",
      "295 815 0.3619631901840491\n",
      "301 815 0.3693251533742331\n",
      "309 815 0.3791411042944785\n",
      "317 815 0.3889570552147239\n",
      "324 815 0.39754601226993863\n",
      "330 815 0.4049079754601227\n",
      "336 815 0.41226993865030676\n",
      "342 815 0.4196319018404908\n",
      "348 815 0.4269938650306748\n",
      "355 815 0.43558282208588955\n",
      "362 815 0.4441717791411043\n",
      "370 815 0.4539877300613497\n",
      "376 815 0.46134969325153374\n",
      "381 815 0.46748466257668714\n",
      "389 815 0.47730061349693254\n",
      "394 815 0.4834355828220859\n",
      "401 815 0.4920245398773006\n",
      "406 815 0.498159509202454\n",
      "411 815 0.5042944785276073\n",
      "417 815 0.5116564417177915\n",
      "423 815 0.5190184049079755\n",
      "430 815 0.5276073619631901\n",
      "436 815 0.5349693251533743\n",
      "441 815 0.5411042944785276\n",
      "449 815 0.550920245398773\n",
      "455 815 0.558282208588957\n",
      "460 815 0.5644171779141104\n",
      "467 815 0.5730061349693252\n",
      "473 815 0.5803680981595092\n",
      "478 815 0.5865030674846625\n",
      "483 815 0.592638036809816\n",
      "490 815 0.6012269938650306\n",
      "498 815 0.6110429447852761\n",
      "503 815 0.6171779141104294\n",
      "510 815 0.6257668711656442\n",
      "514 815 0.6306748466257669\n",
      "519 815 0.6368098159509202\n",
      "526 815 0.645398773006135\n",
      "532 815 0.652760736196319\n",
      "540 815 0.6625766871165644\n",
      "548 815 0.6723926380368098\n",
      "554 815 0.6797546012269938\n",
      "561 815 0.6883435582822086\n",
      "569 815 0.698159509202454\n",
      "574 815 0.7042944785276074\n",
      "581 815 0.7128834355828221\n",
      "585 815 0.7177914110429447\n",
      "590 815 0.7239263803680982\n",
      "595 815 0.7300613496932515\n",
      "601 815 0.7374233128834355\n",
      "609 815 0.747239263803681\n",
      "613 815 0.7521472392638037\n",
      "619 815 0.7595092024539877\n",
      "622 815 0.7631901840490798\n",
      "625 815 0.7668711656441718\n",
      "631 815 0.7742331288343558\n",
      "637 815 0.7815950920245399\n",
      "642 815 0.7877300613496933\n",
      "647 815 0.7938650306748466\n",
      "Finished in 30.85 seconds.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "# instructions = json.load(open(\"data/alpaca_data.json\"))\n",
    "instructions = []\n",
    "with open(data_path, 'r') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        data_point = {}\n",
    "        data_point[\"instruction\"] = 'What is the sentiment toward Bitcoin in the input sentence? [positive, negative, neutral]'\n",
    "        data_point[\"input\"] = data['text']\n",
    "        data_point[\"output\"] = data['label']\n",
    "        full_prompt = prompter.generate_prompt(\n",
    "            data_point[\"instruction\"],\n",
    "            data_point[\"input\"],\n",
    "            '',\n",
    "        )\n",
    "        instructions.append({'context':full_prompt, 'target':data['label']})\n",
    "\n",
    "print(instructions[0])\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    right = 0\n",
    "    all = 0\n",
    "    batch_size = 8\n",
    "    input_texts = []\n",
    "    targets = []\n",
    "    for idx, item in enumerate(instructions[300:]):\n",
    "        # feature = format_example(item)\n",
    "        # input_text = feature[\"context\"]\n",
    "        all = all + 1\n",
    "        input_texts.append(item[\"context\"])\n",
    "        targets.append(item[\"target\"])\n",
    "    test_loader = DataLoader(input_texts, batch_size=batch_size)\n",
    "    for batch_idx,batch in enumerate(test_loader):\n",
    "        input_ids = tokenizer(batch, padding=True,return_tensors='pt').to('cuda')\n",
    "        # out = model.generate(**input_ids, max_length=250, temperature=0)\n",
    "        out = model.generate(\n",
    "            **input_ids,\n",
    "            temperature=0,\n",
    "            return_dict_in_generate= True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens = 1\n",
    "        )\n",
    "        seqs = out['sequences']\n",
    "        scores = out['scores']\n",
    "        # print(scores[0].shape)\n",
    "        results = tokenizer.batch_decode(seqs)\n",
    "        # print(results, '\\n\\n')\n",
    "        # break\n",
    "        for idx,res in enumerate(results):\n",
    "            # print(res,'\\n\\n')\n",
    "            pred = res[res.find('Response') + 10:]\n",
    "            \n",
    "            target = targets[batch_idx*batch_size + idx]\n",
    "            # print(pred, target)\n",
    "#             # print(pred, batch_idx*batch_size + idx,targets[batch_idx*batch_size + idx])\\\n",
    "            # if pred.find(target) >= 0:\n",
    "            #     right = right + 1\n",
    "            if target.find(pred) >= 0:\n",
    "                right = right + 1\n",
    "        print(right,all,right/all)\n",
    "        # break\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Finished in {:.2f} seconds.\".format(elapsed_time))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ddc766-b1da-43f2-9a42-d4aaa04337d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
