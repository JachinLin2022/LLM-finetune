{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b0162a7-b7f9-4904-9194-920596ec063e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /root/miniconda3/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /root/miniconda3/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('Asia/Shanghai')}\n",
      "  warn(msg)\n",
      "/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "\"\"\"\n",
    "Unused imports:\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "\"\"\"\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "from utils.prompter import Prompter\n",
    "\n",
    "base_model = \"/root/llama-7b-hf\"  # the only required argument\n",
    "data_path = \"train_data_3_class_clean.jsonl\"\n",
    "output_dir = \"/root/autodl-tmp/output2s\"\n",
    "# training hyperparams\n",
    "batch_size = 128\n",
    "micro_batch_size = 4\n",
    "num_epochs = 100\n",
    "learning_rate = 3e-4\n",
    "cutoff_len = 256\n",
    "val_set_size = 0\n",
    "# lora hyperparams\n",
    "lora_r = 8\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.05\n",
    "lora_target_modules = [\n",
    "    \"q_proj\",\n",
    "    \"v_proj\",\n",
    "]\n",
    "# llm hyperparams\n",
    "train_on_inputs = True  # if False, masks out inputs in loss\n",
    "add_eos_token = False\n",
    "group_by_length = False  # faster, but produces an odd training loss curve\n",
    "# wandb params\n",
    "wandb_project = \"\"\n",
    "wandb_run_name = \"\"\n",
    "wandb_watch = \"\"  # options: false | gradients | all\n",
    "wandb_log_model = \"\"  # options: false | true\n",
    "# resume_from_checkpoint = '/root/autodl-tmp/output/checkpoint-100'  # either training checkpoint or final adapter\n",
    "resume_from_checkpoint=None\n",
    "prompt_template_name = \"alpaca\"  # The prompt template to use, will default to alpaca.\n",
    "device_map = \"auto\"\n",
    "gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "\n",
    "world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "ddp = world_size != 1\n",
    "use_wandb = len(wandb_project) > 0 or (\n",
    "    \"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3f8eb21-62f8-4c89-9135-92d1d4c0b30c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompter = Prompter(prompt_template_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1338b894-0da3-4c37-8034-9f270048fca2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b230acee00f044e39c7152ce4aefd3c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        load_in_8bit=False,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=device_map,\n",
    "    )\n",
    "# model = LlamaForCausalLM.from_pretrained(\n",
    "#         base_model,\n",
    "#         # load_in_8bit=True,\n",
    "#         # torch_dtype=torch.float16,\n",
    "#         # device_map=device_map,\n",
    "#     ).half().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ccf067c-66d2-44e7-bafe-866d7086673e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "\n",
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    " )\n",
    "tokenizer.padding_side = \"left\"  # Allow batched inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f5cd313-b8b0-4423-bbe4-923c42aef09d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4194304 || all params: 6742609920 || trainable%: 0.06220594176090199\n"
     ]
    }
   ],
   "source": [
    "# model = prepare_model_for_int8_training(model)\n",
    "config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=lora_target_modules,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "if resume_from_checkpoint:\n",
    "    # Check the available weights and load them\n",
    "    checkpoint_name = os.path.join(\n",
    "        resume_from_checkpoint, \"pytorch_model.bin\"\n",
    "    )  # Full checkpoint\n",
    "    if not os.path.exists(checkpoint_name):\n",
    "        checkpoint_name = os.path.join(\n",
    "            resume_from_checkpoint, \"adapter_model.bin\"\n",
    "        )  # only LoRA model - LoRA config above has to fit\n",
    "        resume_from_checkpoint = (\n",
    "            False  # So the trainer won't try loading its state\n",
    "        )\n",
    "    # The two files above have a different name depending on how they were saved, but are actually the same.\n",
    "    if os.path.exists(checkpoint_name):\n",
    "        print(f\"Restarting from {checkpoint_name}\")\n",
    "        adapters_weights = torch.load(checkpoint_name)\n",
    "        set_peft_model_state_dict(model, adapters_weights)\n",
    "    else:\n",
    "        print(f\"Checkpoint {checkpoint_name} not found\")\n",
    "\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94ef226a-560a-4ebf-83fd-8f47a9bf8cca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-c18f34d3d9ca0ab9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a96a65235604d03b54b5edf47847bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-c18f34d3d9ca0ab9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-32aabb3f5ee4accb.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-c18f34d3d9ca0ab9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-8d99697f064efea7.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['instruction', 'input', 'output', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 300\n",
      "})\n",
      "815 Dataset({\n",
      "    features: ['instruction', 'input', 'output', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 815\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def tokenize(prompt, add_eos_token=True):\n",
    "        # there's probably a way to do this with the tokenizer settings\n",
    "        # but again, gotta move fast\n",
    "        result = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=cutoff_len,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        if (\n",
    "            result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "            and len(result[\"input_ids\"]) < cutoff_len\n",
    "            and add_eos_token\n",
    "        ):\n",
    "            result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "            result[\"attention_mask\"].append(1)\n",
    "\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "        return result\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    data_point[\"instruction\"] = 'What is the sentiment toward Bitcoin in the input sentence? [positive, negative, neutral]'\n",
    "    data_point[\"input\"] = data_point['text']\n",
    "    data_point[\"output\"] = data_point['label']\n",
    "    del data_point['text']\n",
    "    del data_point['label']\n",
    "    full_prompt = prompter.generate_prompt(\n",
    "        data_point[\"instruction\"],\n",
    "        data_point[\"input\"],\n",
    "        data_point[\"output\"],\n",
    "    )\n",
    "    # print(full_prompt)\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    if not train_on_inputs:\n",
    "        user_prompt = prompter.generate_prompt(\n",
    "            data_point[\"instruction\"], data_point[\"input\"]\n",
    "        )\n",
    "        tokenized_user_prompt = tokenize(\n",
    "            user_prompt, add_eos_token=add_eos_token\n",
    "        )\n",
    "        user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "\n",
    "        if add_eos_token:\n",
    "            user_prompt_len -= 1\n",
    "\n",
    "        tokenized_full_prompt[\"labels\"] = [\n",
    "            -100\n",
    "        ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
    "            user_prompt_len:\n",
    "        ]  # could be sped up, probably\n",
    "    return tokenized_full_prompt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = load_dataset(\"json\", data_files=data_path)\n",
    "\n",
    "data[\"test\"] = data[\"train\"].select(range(300,len(data[\"train\"])))\n",
    "data[\"train\"] = data[\"train\"].select(range(300))\n",
    "# print(data[\"train\"][0])\n",
    "train_data = data[\"train\"].map(generate_and_tokenize_prompt)\n",
    "val_data = data[\"test\"].map(generate_and_tokenize_prompt)\n",
    "val_set_size = len(val_data)\n",
    "# val_set_size = 0\n",
    "print(train_data)\n",
    "print(val_set_size, val_data)\n",
    "# train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6890a883-6b6f-4c39-a17e-8de05c55f7c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "def my_evaluate(self, ignore_keys):\n",
    "    self.model.eval()    \n",
    "    instructions = []\n",
    "    with open(data_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            data_point = {}\n",
    "            data_point[\"instruction\"] = 'What is the sentiment toward Bitcoin in the input sentence? [positive, negative, neutral]'\n",
    "            data_point[\"input\"] = data['text']\n",
    "            data_point[\"output\"] = data['label']\n",
    "            full_prompt = prompter.generate_prompt(\n",
    "                data_point[\"instruction\"],\n",
    "                data_point[\"input\"]\n",
    "            )\n",
    "            instructions.append({'context':full_prompt, 'target':data['label']})\n",
    "\n",
    "        # print(instructions[0])\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            right = 0\n",
    "            all = 0\n",
    "            batch_size = 32\n",
    "            input_texts = []\n",
    "            targets = []\n",
    "            for idx, item in enumerate(instructions[300:]):\n",
    "                # feature = format_example(item)\n",
    "                # input_text = feature[\"context\"]\n",
    "                all = all + 1\n",
    "                input_texts.append(item[\"context\"])\n",
    "                targets.append(item[\"target\"])\n",
    "            test_loader = DataLoader(input_texts, batch_size=batch_size)\n",
    "            for batch_idx,batch in enumerate(test_loader):\n",
    "                input_ids = tokenizer(batch, padding=True,return_tensors='pt').to('cuda')\n",
    "                out = model.generate(\n",
    "                    **input_ids,\n",
    "                    temperature=0,\n",
    "                    return_dict_in_generate= True,\n",
    "                    output_scores=True,\n",
    "                    max_new_tokens = 1\n",
    "                )\n",
    "                seqs = out['sequences']\n",
    "                scores = out['scores']\n",
    "                # print(scores[0].shape)\n",
    "                results = tokenizer.batch_decode(seqs)\n",
    "                # print(results, '\\n\\n')\n",
    "                # break\n",
    "                for idx,res in enumerate(results):\n",
    "                    pred = res[res.find('Response') + 10:]\n",
    "                    target = targets[batch_idx*batch_size + idx]\n",
    "                    if target.find(pred) >= 0:\n",
    "                        right = right + 1\n",
    "                # print(right,all,right/all)\n",
    "    metrics = {\"eval_acc\": right/all}\n",
    "    self.log(metrics)\n",
    "    # print(metrics)\n",
    "    self.model.train()\n",
    "    self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, metrics)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "\n",
    "transformers.Trainer.evaluate = my_evaluate\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=micro_batch_size,\n",
    "        gradient_accumulation_steps=1,\n",
    "        # warmup_steps=100,\n",
    "        num_train_epochs=num_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        fp16=True,\n",
    "        # logging_strategy  = \"steps\",\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_torch\",\n",
    "        metric_for_best_model = \"acc\",\n",
    "        evaluation_strategy=\"steps\" if val_set_size > 0 else \"no\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=100 if val_set_size > 0 else None,\n",
    "        save_steps=100,\n",
    "        output_dir=output_dir,\n",
    "        save_total_limit=50,\n",
    "        load_best_model_at_end=True if val_set_size > 0 else False,\n",
    "        ddp_find_unused_parameters=False if ddp else None,\n",
    "        group_by_length=group_by_length,\n",
    "        report_to=\"wandb\" if use_wandb else None,\n",
    "        run_name=wandb_run_name if use_wandb else None,\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "        tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "    ),\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "# old_state_dict = model.state_dict\n",
    "# model.state_dict = (\n",
    "#     lambda self, *_, **__: get_peft_model_state_dict(\n",
    "#         self, old_state_dict()\n",
    "#     )\n",
    "# ).__get__(model, type(model))\n",
    "# if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "#     model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3df305b1-398c-40bf-9868-14fd97e17f58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7500/7500 1:22:24, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.991500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.790184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.788700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.814724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.789800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.807362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.438700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.826994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.391700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.828221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.302300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.818405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.181800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.828221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.181200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.817178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.167000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.834356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.145800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.833129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.144400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.828221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.139100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.822086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.128400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.835583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.127000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.830675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.130900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.834356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.120500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.823313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.127900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.829448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.124400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.823313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.123600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.817178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.126500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.830675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.127400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.824540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.115100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.830675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.120700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.824540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.120900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.829448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.115100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.831902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.123700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.826994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.124400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.830675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.117600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.823313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.116100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.818405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.117300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.825767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.125900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.833129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.114700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.824540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.116500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.833129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.112100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.831902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.112200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.830675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.116800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.836810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.109500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.831902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.112700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.833129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.112700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.836810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.120500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.829448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.110400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.834356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.834356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.112200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.830675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.118300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.834356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.114300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.833129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.116300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.833129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.111900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.840491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.114000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.833129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.105900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.834356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.112000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.829448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.110200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.836810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.110500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.834356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.107300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.838037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.115700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.836810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.107800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.838037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.111800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.836810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.841718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.107800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.842945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.108400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.840491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.111000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.836810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.110900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.839264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.108900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.840491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.113700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.839264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.105700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.840491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.110700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.840491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.117600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.840491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.107000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.838037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.836810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.108500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.839264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.106600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.838037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.113800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.840491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.117300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.838037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.108400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.838037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.110100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.839264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.108100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.839264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7500, training_loss=0.1646334870815277, metrics={'train_runtime': 4946.4504, 'train_samples_per_second': 6.065, 'train_steps_per_second': 1.516, 'total_flos': 1.7496858579173376e+17, 'train_loss': 0.1646334870815277, 'epoch': 100.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=resume_from_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9135cf21-cb43-49e5-a397-780dd4b365dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8797a78-2e42-426d-b954-5724b5a86567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat is the sentiment toward Bitcoin in the input sentence? [positive, negative, neutral]\\n\\n### Input:\\nBut... Bitcoin's halving IS priced in.\\n\\nFirst reminder of 2020\\n\\n### Response:\\n\", 'target': 'negative'}\n",
      "6 815 0.007361963190184049\n",
      "14 815 0.01717791411042945\n",
      "21 815 0.025766871165644172\n",
      "25 815 0.03067484662576687\n",
      "31 815 0.03803680981595092\n",
      "35 815 0.04294478527607362\n",
      "42 815 0.051533742331288344\n",
      "49 815 0.06012269938650307\n",
      "56 815 0.0687116564417178\n",
      "64 815 0.0785276073619632\n",
      "72 815 0.08834355828220859\n",
      "78 815 0.09570552147239264\n",
      "86 815 0.10552147239263804\n",
      "94 815 0.11533742331288344\n",
      "101 815 0.12392638036809817\n",
      "108 815 0.1325153374233129\n",
      "114 815 0.13987730061349693\n",
      "122 815 0.14969325153374233\n",
      "130 815 0.15950920245398773\n",
      "134 815 0.16441717791411042\n",
      "142 815 0.17423312883435582\n",
      "150 815 0.18404907975460122\n",
      "156 815 0.19141104294478528\n",
      "162 815 0.19877300613496932\n",
      "169 815 0.20736196319018405\n",
      "175 815 0.2147239263803681\n",
      "183 815 0.2245398773006135\n",
      "191 815 0.2343558282208589\n",
      "199 815 0.2441717791411043\n",
      "206 815 0.252760736196319\n",
      "213 815 0.26134969325153373\n",
      "220 815 0.26993865030674846\n",
      "227 815 0.2785276073619632\n",
      "235 815 0.2883435582822086\n",
      "241 815 0.29570552147239265\n",
      "246 815 0.301840490797546\n",
      "253 815 0.3104294478527607\n",
      "261 815 0.3202453987730061\n",
      "269 815 0.3300613496932515\n",
      "276 815 0.33865030674846625\n",
      "284 815 0.34846625766871164\n",
      "292 815 0.35828220858895704\n",
      "300 815 0.36809815950920244\n",
      "307 815 0.37668711656441717\n",
      "314 815 0.3852760736196319\n",
      "322 815 0.3950920245398773\n",
      "330 815 0.4049079754601227\n",
      "337 815 0.4134969325153374\n",
      "344 815 0.42208588957055215\n",
      "351 815 0.4306748466257669\n",
      "358 815 0.4392638036809816\n",
      "366 815 0.449079754601227\n",
      "374 815 0.4588957055214724\n",
      "382 815 0.4687116564417178\n",
      "390 815 0.4785276073619632\n",
      "396 815 0.48588957055214727\n",
      "403 815 0.49447852760736194\n",
      "410 815 0.5030674846625767\n",
      "415 815 0.50920245398773\n",
      "420 815 0.5153374233128835\n",
      "426 815 0.5226993865030675\n",
      "433 815 0.5312883435582823\n",
      "440 815 0.5398773006134969\n",
      "446 815 0.5472392638036809\n",
      "454 815 0.5570552147239264\n",
      "459 815 0.5631901840490797\n",
      "465 815 0.5705521472392638\n",
      "473 815 0.5803680981595092\n",
      "479 815 0.5877300613496933\n",
      "484 815 0.5938650306748466\n",
      "490 815 0.6012269938650306\n",
      "497 815 0.6098159509202454\n",
      "504 815 0.6184049079754601\n",
      "512 815 0.6282208588957056\n",
      "520 815 0.6380368098159509\n",
      "528 815 0.6478527607361964\n",
      "535 815 0.656441717791411\n",
      "541 815 0.6638036809815951\n",
      "546 815 0.6699386503067485\n",
      "551 815 0.6760736196319018\n",
      "557 815 0.6834355828220859\n",
      "565 815 0.6932515337423313\n",
      "572 815 0.701840490797546\n",
      "578 815 0.7092024539877301\n",
      "584 815 0.7165644171779141\n",
      "592 815 0.7263803680981595\n",
      "600 815 0.7361963190184049\n",
      "608 815 0.7460122699386503\n",
      "615 815 0.754601226993865\n",
      "620 815 0.7607361963190185\n",
      "626 815 0.7680981595092025\n",
      "631 815 0.7742331288343558\n",
      "636 815 0.7803680981595092\n",
      "644 815 0.7901840490797546\n",
      "648 815 0.7950920245398773\n",
      "654 815 0.8024539877300614\n",
      "658 815 0.807361963190184\n",
      "663 815 0.8134969325153374\n",
      "670 815 0.8220858895705522\n",
      "677 815 0.8306748466257668\n",
      "681 815 0.8355828220858895\n",
      "687 815 0.8429447852760736\n",
      "Finished in 30.34 seconds.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "# instructions = json.load(open(\"data/alpaca_data.json\"))\n",
    "instructions = []\n",
    "with open(data_path, 'r') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        data_point = {}\n",
    "        data_point[\"instruction\"] = 'What is the sentiment toward Bitcoin in the input sentence? [positive, negative, neutral]'\n",
    "        data_point[\"input\"] = data['text']\n",
    "        data_point[\"output\"] = data['label']\n",
    "        full_prompt = prompter.generate_prompt(\n",
    "            data_point[\"instruction\"],\n",
    "            data_point[\"input\"]\n",
    "        )\n",
    "        instructions.append({'context':full_prompt, 'target':data['label']})\n",
    "\n",
    "print(instructions[0])\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    right = 0\n",
    "    all = 0\n",
    "    batch_size = 8\n",
    "    input_texts = []\n",
    "    targets = []\n",
    "    for idx, item in enumerate(instructions[300:]):\n",
    "        # feature = format_example(item)\n",
    "        # input_text = feature[\"context\"]\n",
    "        all = all + 1\n",
    "        input_texts.append(item[\"context\"])\n",
    "        targets.append(item[\"target\"])\n",
    "    test_loader = DataLoader(input_texts, batch_size=batch_size)\n",
    "    for batch_idx,batch in enumerate(test_loader):\n",
    "        input_ids = tokenizer(batch, padding=True,return_tensors='pt').to('cuda')\n",
    "        # out = model.generate(**input_ids, max_length=250, temperature=0)\n",
    "        out = model.generate(\n",
    "            **input_ids,\n",
    "            temperature=0,\n",
    "            return_dict_in_generate= True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens = 1\n",
    "        )\n",
    "        seqs = out['sequences']\n",
    "        scores = out['scores']\n",
    "        # print(scores[0].shape)\n",
    "        results = tokenizer.batch_decode(seqs)\n",
    "        # print(results, '\\n\\n')\n",
    "        # break\n",
    "        for idx,res in enumerate(results):\n",
    "            # print(res,'\\n\\n')\n",
    "            pred = res[res.find('Response') + 10:]\n",
    "            \n",
    "            target = targets[batch_idx*batch_size + idx]\n",
    "            # print(pred, target)\n",
    "#             # print(pred, batch_idx*batch_size + idx,targets[batch_idx*batch_size + idx])\\\n",
    "            # if pred.find(target) >= 0:\n",
    "            #     right = right + 1\n",
    "            if target.find(pred) >= 0:\n",
    "                right = right + 1\n",
    "        print(right,all,right/all)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Finished in {:.2f} seconds.\".format(elapsed_time))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5318b897-1a20-4a76-b838-1eedea20a5e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
