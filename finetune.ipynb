{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b0162a7-b7f9-4904-9194-920596ec063e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /root/miniconda3/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /root/miniconda3/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('Asia/Shanghai')}\n",
      "  warn(msg)\n",
      "/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "\"\"\"\n",
    "Unused imports:\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "\"\"\"\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "from utils.prompter import Prompter\n",
    "\n",
    "base_model = \"/root/llama-7b-hf\"  # the only required argument\n",
    "data_path = \"train_data_3_class.jsonl\"\n",
    "output_dir = \"/root/autodl-tmp/8bit\"\n",
    "# training hyperparams\n",
    "batch_size = 128\n",
    "micro_batch_size = 4\n",
    "num_epochs = 50\n",
    "learning_rate = 3e-4\n",
    "cutoff_len = 256\n",
    "val_set_size = 0\n",
    "# lora hyperparams\n",
    "lora_r = 8\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.05\n",
    "lora_target_modules = [\n",
    "    \"q_proj\",\n",
    "    \"v_proj\",\n",
    "]\n",
    "# llm hyperparams\n",
    "train_on_inputs = True  # if False, masks out inputs in loss\n",
    "add_eos_token = False\n",
    "group_by_length = False  # faster, but produces an odd training loss curve\n",
    "# wandb params\n",
    "wandb_project = \"\"\n",
    "wandb_run_name = \"\"\n",
    "wandb_watch = \"\"  # options: false | gradients | all\n",
    "wandb_log_model = \"\"  # options: false | true\n",
    "# resume_from_checkpoint = '/root/autodl-tmp/mask_non_label/checkpoint-1400'  # either training checkpoint or final adapter\n",
    "resume_from_checkpoint=None\n",
    "prompt_template_name = \"alpaca\"  # The prompt template to use, will default to alpaca.\n",
    "device_map = \"auto\"\n",
    "gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "\n",
    "world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "ddp = world_size != 1\n",
    "use_wandb = len(wandb_project) > 0 or (\n",
    "    \"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3f8eb21-62f8-4c89-9135-92d1d4c0b30c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompter = Prompter(prompt_template_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1338b894-0da3-4c37-8034-9f270048fca2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411be7d2a6914fef8e308422338af27a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        load_in_8bit=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=device_map,\n",
    "    )\n",
    "# model = LlamaForCausalLM.from_pretrained(\n",
    "#         base_model,\n",
    "#         # load_in_8bit=True,\n",
    "#         # torch_dtype=torch.float16,\n",
    "#         # device_map=device_map,\n",
    "#     ).half().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ccf067c-66d2-44e7-bafe-866d7086673e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "\n",
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    " )\n",
    "tokenizer.padding_side = \"left\"  # Allow batched inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f5cd313-b8b0-4423-bbe4-923c42aef09d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4194304 || all params: 6742609920 || trainable%: 0.06220594176090199\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model_for_int8_training(model)\n",
    "config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=lora_target_modules,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "if resume_from_checkpoint:\n",
    "    # Check the available weights and load them\n",
    "    checkpoint_name = os.path.join(\n",
    "        resume_from_checkpoint, \"pytorch_model.bin\"\n",
    "    )  # Full checkpoint\n",
    "    if not os.path.exists(checkpoint_name):\n",
    "        checkpoint_name = os.path.join(\n",
    "            resume_from_checkpoint, \"adapter_model.bin\"\n",
    "        )  # only LoRA model - LoRA config above has to fit\n",
    "        resume_from_checkpoint = (\n",
    "            False  # So the trainer won't try loading its state\n",
    "        )\n",
    "    # The two files above have a different name depending on how they were saved, but are actually the same.\n",
    "    if os.path.exists(checkpoint_name):\n",
    "        print(f\"Restarting from {checkpoint_name}\")\n",
    "        adapters_weights = torch.load(checkpoint_name)\n",
    "        set_peft_model_state_dict(model, adapters_weights)\n",
    "    else:\n",
    "        print(f\"Checkpoint {checkpoint_name} not found\")\n",
    "\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cd0909-ecc3-421b-b95e-691f965df89e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94ef226a-560a-4ebf-83fd-8f47a9bf8cca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-8c9e3f1acfd4e318/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd409aaf79f7478484f739824ddbc709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8c9e3f1acfd4e318/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-338a3ee77e6b34a2.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8c9e3f1acfd4e318/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-ec996a6eb44b355f.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['instruction', 'input', 'output', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 300\n",
      "})\n",
      "Dataset({\n",
      "    features: ['instruction', 'input', 'output', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 700\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def tokenize(prompt, add_eos_token=True):\n",
    "        # there's probably a way to do this with the tokenizer settings\n",
    "        # but again, gotta move fast\n",
    "        result = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=cutoff_len,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        if (\n",
    "            result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "            and len(result[\"input_ids\"]) < cutoff_len\n",
    "            and add_eos_token\n",
    "        ):\n",
    "            result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "            result[\"attention_mask\"].append(1)\n",
    "\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "        return result\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    data_point[\"instruction\"] = 'What is the sentiment toward Bitcoin in the input sentence? [positive, negative, neutral]'\n",
    "    data_point[\"input\"] = data_point['text']\n",
    "    data_point[\"output\"] = data_point['label']\n",
    "    del data_point['text']\n",
    "    del data_point['label']\n",
    "    full_prompt = prompter.generate_prompt(\n",
    "        data_point[\"instruction\"],\n",
    "        data_point[\"input\"],\n",
    "        data_point[\"output\"],\n",
    "    )\n",
    "    # print(full_prompt)\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    if not train_on_inputs:\n",
    "        user_prompt = prompter.generate_prompt(\n",
    "            data_point[\"instruction\"], data_point[\"input\"]\n",
    "        )\n",
    "        tokenized_user_prompt = tokenize(\n",
    "            user_prompt, add_eos_token=add_eos_token\n",
    "        )\n",
    "        user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "\n",
    "        if add_eos_token:\n",
    "            user_prompt_len -= 1\n",
    "\n",
    "        tokenized_full_prompt[\"labels\"] = [\n",
    "            -100\n",
    "        ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
    "            user_prompt_len:\n",
    "        ]  # could be sped up, probably\n",
    "    return tokenized_full_prompt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = load_dataset(\"json\", data_files=data_path)\n",
    "\n",
    "data[\"test\"] = data[\"train\"].select(range(300,1000))\n",
    "data[\"train\"] = data[\"train\"].select(range(300))\n",
    "# print(data[\"train\"][0])\n",
    "train_data = data[\"train\"].map(generate_and_tokenize_prompt)\n",
    "val_data = data[\"test\"].map(generate_and_tokenize_prompt)\n",
    "print(train_data)\n",
    "print(val_data)\n",
    "# train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e7940f2-3dcc-4940-9705-66911a161adb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# from torch.utils.data import DataLoader\n",
    "# import time\n",
    "\n",
    "# # instructions = json.load(open(\"data/alpaca_data.json\"))\n",
    "# instructions = []\n",
    "# with open(data_path, 'r') as f:\n",
    "#     for line in f:\n",
    "#         data = json.loads(line)\n",
    "#         data_point = {}\n",
    "#         data_point[\"instruction\"] = 'What is the sentiment toward Bitcoin in the input sentence? [positive, negative, neutral]'\n",
    "#         data_point[\"input\"] = data['text']\n",
    "#         data_point[\"output\"] = data['label']\n",
    "#         full_prompt = prompter.generate_prompt(\n",
    "#             data_point[\"instruction\"],\n",
    "#             data_point[\"input\"],\n",
    "#             '',\n",
    "#         )\n",
    "#         instructions.append({'context':full_prompt, 'target':data['label']})\n",
    "\n",
    "# print(instructions[0])\n",
    "# start_time = time.time()\n",
    "# with torch.no_grad():\n",
    "#     right = 0\n",
    "#     all = 0\n",
    "#     batch_size = 8\n",
    "#     input_texts = []\n",
    "#     targets = []\n",
    "#     for idx, item in enumerate(instructions[300:]):\n",
    "#         # feature = format_example(item)\n",
    "#         # input_text = feature[\"context\"]\n",
    "#         all = all + 1\n",
    "#         input_texts.append(item[\"context\"])\n",
    "#         targets.append(item[\"target\"])\n",
    "#     test_loader = DataLoader(input_texts, batch_size=batch_size)\n",
    "#     for batch_idx,batch in enumerate(test_loader):\n",
    "#         input_ids = tokenizer(batch, padding=True,return_tensors='pt').to('cuda')\n",
    "#         # out = model.generate(**input_ids, max_length=250, temperature=0)\n",
    "#         out = model.generate(\n",
    "#             **input_ids,\n",
    "#             temperature=0,\n",
    "#             return_dict_in_generate= True,\n",
    "#             output_scores=True,\n",
    "#             # max_new_tokens = 128\n",
    "#         )\n",
    "#         seqs = out['sequences']\n",
    "#         scores = out['scores']\n",
    "#         # print(scores[0].shape)\n",
    "#         results = tokenizer.batch_decode(seqs)\n",
    "#         # print(results, '\\n\\n')\n",
    "#         # break\n",
    "#         for idx,res in enumerate(results):\n",
    "#             # print(res,'\\n\\n')\n",
    "#             pred = res[res.find('Response') + 10:]\n",
    "            \n",
    "#             target = targets[batch_idx*batch_size + idx]\n",
    "#             print(pred, target)\n",
    "# #             # print(pred, batch_idx*batch_size + idx,targets[batch_idx*batch_size + idx])\\\n",
    "#             # if pred.find(target) >= 0:\n",
    "#             #     right = right + 1\n",
    "#             if target.find(pred) >= 0:\n",
    "#                 right = right + 1\n",
    "#         print(right,all,right/all)\n",
    "# end_time = time.time()\n",
    "# elapsed_time = end_time - start_time\n",
    "# print(\"Finished in {:.2f} seconds.\".format(elapsed_time))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6890a883-6b6f-4c39-a17e-8de05c55f7c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=micro_batch_size,\n",
    "        gradient_accumulation_steps=1,\n",
    "        # warmup_steps=100,\n",
    "        num_train_epochs=num_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_torch\",\n",
    "        evaluation_strategy=\"no\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=5,\n",
    "        save_steps=100,\n",
    "        output_dir=output_dir,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True if val_set_size > 0 else False,\n",
    "        ddp_find_unused_parameters=False if ddp else None,\n",
    "        group_by_length=group_by_length,\n",
    "        report_to=\"wandb\" if use_wandb else None,\n",
    "        run_name=wandb_run_name if use_wandb else None,\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "        tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "    ),\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "old_state_dict = model.state_dict\n",
    "model.state_dict = (\n",
    "    lambda self, *_, **__: get_peft_model_state_dict(\n",
    "        self, old_state_dict()\n",
    "    )\n",
    ").__get__(model, type(model))\n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df305b1-398c-40bf-9868-14fd97e17f58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  70/3750 00:43 < 39:21, 1.56 it/s, Epoch 0.92/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.094300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.910800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.068400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.149100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.905300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.877700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.816300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.626900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.607400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.401100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.496300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.636600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.827800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.982600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.139300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.522500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.604500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.938300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.959700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.622800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.607400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.478500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.895800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.384800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.257500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.561200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.376100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.624800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.263300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.394700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.440800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.271800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.238000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.345900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.276400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.284700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.394400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.233200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.270700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.315600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.210400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.124700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.412100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.444000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.234100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.261700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.335700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.338200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.362500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.295000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.317100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.184700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.264600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.206900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.124500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.228600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.419800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.007500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.213700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.056700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.091200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.231100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.028300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.215100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.229600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=resume_from_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9135cf21-cb43-49e5-a397-780dd4b365dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8797a78-2e42-426d-b954-5724b5a86567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "# instructions = json.load(open(\"data/alpaca_data.json\"))\n",
    "instructions = []\n",
    "with open('train_data_3_class.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        data_point = {}\n",
    "        data_point[\"instruction\"] = 'What is the sentiment toward Bitcoin in the input sentence? [positive, negative, neutral]'\n",
    "        data_point[\"input\"] = data['text']\n",
    "        data_point[\"output\"] = data['label']\n",
    "        full_prompt = prompter.generate_prompt(\n",
    "            data_point[\"instruction\"],\n",
    "            data_point[\"input\"],\n",
    "            '',\n",
    "        )\n",
    "        instructions.append({'context':full_prompt, 'target':data['label']})\n",
    "\n",
    "print(instructions[0])\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    right = 0\n",
    "    all = 0\n",
    "    batch_size = 8\n",
    "    input_texts = []\n",
    "    targets = []\n",
    "    for idx, item in enumerate(instructions[300:]):\n",
    "        # feature = format_example(item)\n",
    "        # input_text = feature[\"context\"]\n",
    "        all = all + 1\n",
    "        input_texts.append(item[\"context\"])\n",
    "        targets.append(item[\"target\"])\n",
    "    test_loader = DataLoader(input_texts, batch_size=batch_size)\n",
    "    for batch_idx,batch in enumerate(test_loader):\n",
    "        input_ids = tokenizer(batch, padding=True,return_tensors='pt').to('cuda')\n",
    "        # out = model.generate(**input_ids, max_length=250, temperature=0)\n",
    "        out = model.generate(\n",
    "            **input_ids,\n",
    "            temperature=0,\n",
    "            return_dict_in_generate= True,\n",
    "            output_scores=True,\n",
    "            # max_new_tokens = 20\n",
    "        )\n",
    "        seqs = out['sequences']\n",
    "        scores = out['scores']\n",
    "        # print(scores[0].shape)\n",
    "        results = tokenizer.batch_decode(seqs)\n",
    "        # print(results, '\\n\\n')\n",
    "        # break\n",
    "        for idx,res in enumerate(results):\n",
    "            # print(res,'\\n\\n')\n",
    "            pred = res[res.find('Response') + 10:]\n",
    "            \n",
    "            target = targets[batch_idx*batch_size + idx]\n",
    "            # print(pred, target)\n",
    "#             # print(pred, batch_idx*batch_size + idx,targets[batch_idx*batch_size + idx])\\\n",
    "            # if pred.find(target) >= 0:\n",
    "            #     right = right + 1\n",
    "            if target.find(pred) >= 0:\n",
    "                right = right + 1\n",
    "        print(right,all,right/all)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Finished in {:.2f} seconds.\".format(elapsed_time))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5318b897-1a20-4a76-b838-1eedea20a5e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
